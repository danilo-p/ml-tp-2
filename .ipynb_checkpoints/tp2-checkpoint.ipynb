{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Prático 2 - Implemetação do Algoritmo de Boosting\n",
    "\n",
    "Disciplina: Aprendizado de Máquina\n",
    "\n",
    "Aluno: Danilo Pimentel de Carvalho Costa\n",
    "\n",
    "Matrícula: 2016058077\n",
    "\n",
    "## Introdução\n",
    "\n",
    "Duas tentativas de implementar o algoritmo de boosting foram feitas. Como será mostrado a seguir, nenhuma das duas implementações teve grande successo em termos de acurácia. Os resultados serão discutidos nas seções a seguir.\n",
    "\n",
    "As duas implementações utilizam as ferramentas que a biblioteca sklearn fornece para construção de classificadores. Tais ferramentas foram convenientes para utilizar as funções de avaliação de modelos que a biblioteca também fornece. A referência consultada se encontra nesta página: https://scikit-learn.org/stable/developers/develop.html#rolling-your-own-estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "# Method used to evaluate performance of the classifiers.\n",
    "# Reference: https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics\n",
    "def eval_classifier_performance(clf, X, y):\n",
    "    scores = cross_val_score(clf, X, y, cv=5)\n",
    "    print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentativa 1 - Abordagem ingênua\n",
    "\n",
    "A primeira tentativa de implementação adota uma estratégia ingênua, utilizando estruturas de repetição, condições manuais, e lambdas em seu funcionamento. Tal implementação se provou simples de se construir, porém muito lenta em termos de performance. A hipótese é de que todas as estruturas de repetição, checagens e chamadas de função lambda adicionam um fardo na execução. A má performance mostrada na subseção \"Avaliação e Performance\" tornou inviável sua depuração e avaliação.\n",
    "\n",
    "### Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, -1]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tp2ClassifierTake1(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_stumps=10):\n",
    "        self.n_stumps = n_stumps\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y, dtype='str')\n",
    "\n",
    "        self.X_ = pd.DataFrame(X)\n",
    "        self.y_ = y\n",
    "        \n",
    "        # Create stumps based on the categories found for each feature...\n",
    "        stumps = []\n",
    "        ## ...iterating over the features of the dataset...\n",
    "        for (name, column) in self.X_.iteritems():\n",
    "            # ...getting the unique categories for each feature...\n",
    "            values = column.unique()\n",
    "            for value in values:\n",
    "                # ...and finally creating lambda functions to return true when\n",
    "                # the example has the given category for the given feature...\n",
    "                stumps.append(lambda c: 1 if c[name] == value else -1)\n",
    "                # ...and the same for the ausence of the given category for the\n",
    "                # given feature.\n",
    "                stumps.append(lambda c: 1 if c[name] != value else -1)\n",
    "        # Also create lambdas for the stumps that return fixed values.\n",
    "        stumps.append(lambda _: 1)\n",
    "        stumps.append(lambda _: -1)\n",
    "\n",
    "        # The number of given examples.\n",
    "        m = len(y)\n",
    "\n",
    "        # We then create the weights array, with the same weigth for all the\n",
    "        # examples. This array is going to be updated according to the mistakes\n",
    "        # made the selected stump on each iteration.\n",
    "        w = [1 / m] * m\n",
    "        # The array below will store tupls of the stumps (lambdas) that we\n",
    "        # select during and the respective importance.\n",
    "        selected_stumps = []\n",
    "        for i in range(self.n_stumps):\n",
    "            # We look for the best stump by calculating the weighted empirical\n",
    "            # error.\n",
    "            best_stump = None\n",
    "            best_stump_empirical_error = -1\n",
    "            for stump in stumps:\n",
    "                empirical_error = 0\n",
    "                for k, item in self.X_.iterrows():\n",
    "                    if stump(item) != y[k]:\n",
    "                        empirical_error += w[k]\n",
    "                if best_stump == None or best_stump_empirical_error > empirical_error:\n",
    "                    best_stump = stump\n",
    "                    best_stump_empirical_error = empirical_error\n",
    "\n",
    "            # After finding the best stump for the current weights, we calculate its\n",
    "            # importance.\n",
    "            if best_stump_empirical_error == 0:\n",
    "                # If it happens, we found a strong classifier for these examples. We\n",
    "                # consider its importance as 1, and stop looking for more stumps.\n",
    "                selected_stumps.append((1, best_stump))\n",
    "                break\n",
    "            else:\n",
    "                # If the stump made mistakes, we calculate the importance regularly.\n",
    "                alpha = (1 / 2) * math.log((1 - best_stump_empirical_error) / best_stump_empirical_error)\n",
    "                selected_stumps.append((alpha, best_stump))\n",
    "\n",
    "            # Knowing the selected stump mistakes, we can now update the weights so we\n",
    "            # choose a stump that is good on what the selected stump is bad.\n",
    "            w_total = 0\n",
    "            for j, item in self.X_.iterrows():\n",
    "                w[j] *= math.exp(-1 * alpha * best_stump(item) * y[j])\n",
    "                w_total += w[j]\n",
    "            for j in range(m):\n",
    "                w[j] /= w_total\n",
    "\n",
    "        # We store the selected stumps for futher prediction.\n",
    "        self.selected_stumps_ = selected_stumps\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called.\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation.\n",
    "        X = check_array(X, ensure_2d=False, dtype='str')\n",
    "\n",
    "        # Make predictions for each example contained on X.\n",
    "        y_predicted = []\n",
    "        for i, item in pd.DataFrame(X).iterrows():\n",
    "            # Calculate the score based on the selected stump responses\n",
    "            # considering its importances.\n",
    "            score = 0\n",
    "            for selected_stump in self.selected_stumps_:\n",
    "                alpha, stump = selected_stump\n",
    "                score += alpha * stump(item)\n",
    "\n",
    "            # If the final score is negative, the predict negative. Predict\n",
    "            # positive otherwise.\n",
    "            prediction = -1 if score < 0 else 1\n",
    "            y_predicted.append(prediction)\n",
    "        \n",
    "        return y_predicted\n",
    "\n",
    "# Create some simple data for testing the implementation\n",
    "dummy_X = np.array([\n",
    "    ['x', 'o'],\n",
    "    ['o', 'o'],\n",
    "    ['o', 'x']\n",
    "])\n",
    "dummy_y = np.array([\n",
    "    1,\n",
    "    -1,\n",
    "    1\n",
    "])\n",
    "dummy_predict_X = np.array([\n",
    "    ['x', 'x'],\n",
    "    ['o', 'o']\n",
    "])\n",
    "\n",
    "Tp2ClassifierTake1(n_stumps=1).fit(dummy_X, dummy_y).predict(dummy_predict_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação e Performance\n",
    "\n",
    "Não foi preciso uma análise mais elaborada para entender que o classificador implementado acima não tem boa performance. Mesmo configurando para selecionar somente 1 stump, a execução da validação cruzada leva mais de 10 segundos. Aumentando para 5 stumps, o tempo de execução salta para mais de 1 minuto. Os resultados obtidos nas validações cruzadas executadas não são conclusivos, mas a evidência de má performance é suficiente para motivar a busca por uma melhor implementação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tic tac toe dataset\n",
    "tic_tac_toe_data = pd.read_csv('./tic-tac-toe/tic-tac-toe.data', header=None)\n",
    "\n",
    "# Create the matrix X with the examples\n",
    "X = tic_tac_toe_data.drop([9], axis=1)\n",
    "\n",
    "# Transform the classes from strings to -1s and 1s\n",
    "y = tic_tac_toe_data[9]\n",
    "y = label_binarize(y, classes=['negative', 'positive'], pos_label = 1, neg_label = -1)\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validação Cruzada 5-fold selecionando 1 stump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.00)\n",
      "CPU times: user 14.8 s, sys: 22.3 ms, total: 14.8 s\n",
      "Wall time: 14.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake1(n_stumps=1), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validação Cruzada 5-fold selecionando 5 stumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.00)\n",
      "CPU times: user 1min 13s, sys: 106 ms, total: 1min 13s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake1(n_stumps=5), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tentativa 2 - Abordagem utilizando matrizes\n",
    "\n",
    "Para melhorar o tempo de treinamento e predição do classificador, a implementação foi repensada para utilizar a boa performance de operações matriciais utilizando a biblioteca numpy. Os ganhos de performance foram expressivos, o que possibilitou a melhor depuração, evolução, e avaliação do algoritmo.\n",
    "\n",
    "### Implementação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Tp2ClassifierTake2(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_stumps=10):\n",
    "        self.n_stumps = n_stumps\n",
    "        \n",
    "    # X is the dataset. S is the matrix with stumps for each feature. See the fit\n",
    "    # method for the description of how to build S.\n",
    "    def compute_stump_predictions_(self, X, S):\n",
    "        # Compute all stumps predictions for all examples given. We will try to build\n",
    "        # a matrix P that will contain 0s indicating that the stump returned false for\n",
    "        # example, and 1 otherwise.\n",
    "        P = np.matmul(X, np.transpose(S)) # dimensions: m x [amount of stumps]\n",
    "        # Here we select the values that indicate stumps returning true\n",
    "        P = np.ma.masked_array(\n",
    "            P,\n",
    "            # We are removing from P...\n",
    "            np.logical_and(\n",
    "                # ...category-presence stumps returning false...\n",
    "                P != 1,\n",
    "                # ...and category-ausence stumps return false.\n",
    "                np.logical_not(np.logical_and(P < 0, P != -1))\n",
    "            )\n",
    "        )\n",
    "        # We fill the values we removed with 0s to indicate that the stumps returned\n",
    "        # false...\n",
    "        P = np.ma.filled(P, fill_value=0)\n",
    "        # ...everything else with 1s to indicate that the stump returned true.\n",
    "        P[P != 0] = 1\n",
    "        \n",
    "        # The part of setting the predictions for stumps that return always true or\n",
    "        # always false is left for the method caller, as it changes between the fit\n",
    "        # and the predict methods.\n",
    "        return P\n",
    "\n",
    "    # We are assuming here that X will be a matrix of positive integers (no zeros,\n",
    "    # no negatives). A good enhancement to be made here is to accept any kind of class,\n",
    "    # and then process it to fit the expectations mentioned. y needs to be a binary\n",
    "    # vector (0s and 1s).\n",
    "    def fit(self, X, y):\n",
    "        # Check that X and y have correct shape.\n",
    "        X, y = check_X_y(X, y, dtype='numeric')\n",
    "        \n",
    "        # Number of examples and features.\n",
    "        m, n = X.shape\n",
    "\n",
    "        # Number of unique categories.\n",
    "        c = len(np.unique(X))\n",
    "\n",
    "        # The amount of stumps: there are two stumps for each category, and we need the\n",
    "        # stumps for each feature. We also need two stumps with fixed return value.\n",
    "        # Therefore, 2 * number of categories * number of features + 2 fixed stumps.\n",
    "        S = np.zeros((2 * c * n + 2, n))\n",
    "\n",
    "        # Create the stumps that respond true when the example has a given category for\n",
    "        # a given feature.\n",
    "        for j in range(n):\n",
    "            for k in range(c):\n",
    "                # The trick here is that we will multiply this value with the example\n",
    "                # vector, so if on the given vector, the feature j has the category\n",
    "                # k + 1, then the result of the multiplication will be 1. Otherwise,\n",
    "                # it will be something else we can ignore.\n",
    "                S[j * c + k, j] = 1 / (k + 1)\n",
    "\n",
    "        # Create the stumps that respond true when the example does not have a given\n",
    "        # category for a given feature.\n",
    "        for j in range(n):\n",
    "            for k in range(c):\n",
    "                # Following the same thought process, if on the given vector, the\n",
    "                # feature j has the category k + 1, then the result of the\n",
    "                # multiplication will be -1. Otherwise, it will be something else\n",
    "                # we can USE. We will use any negative result different from -1 to\n",
    "                # know that the category k + 1 isn't in there.\n",
    "                S[(j * c + k) + c * n, j] = -1 / (k + 1)\n",
    "                \n",
    "        # Compute all stumps predictions for all examples given. Check the comments\n",
    "        # on compute_stump_predictions_ to understand how we do it.\n",
    "        P = self.compute_stump_predictions_(X, S)\n",
    "        # We set the output for the two stumps that always return true and\n",
    "        # false. These are the last 2 stumps on the S matrix.\n",
    "        P[:, -2] = 0\n",
    "        P[:, -1] = 1\n",
    "\n",
    "        # Now we want to know the mistakes made by the stumps. We build a binary\n",
    "        # matrix E with 0s indicating a correct prediction and 1s indicating errors.\n",
    "        E = (P != y[:, None]) + 0 # dimensions: m x [amount of stumps]\n",
    "\n",
    "        # As this implementation works with reweighting, we will want a vector w to\n",
    "        # store the weights of each example. It starts with equal weight for all.\n",
    "        w = np.array([1 / m] * m)\n",
    "        \n",
    "        # As we select stumps, we need to store its importance and the stump itself.\n",
    "        # The vector A will store the former, while H and Hf will store the latter.\n",
    "        # Hf will indicate if the stump returns a fixed response (1 or -1), or its\n",
    "        # just a regular stump (0).\n",
    "        A = []\n",
    "        H = []\n",
    "        Hf = []\n",
    "        for i in range(self.n_stumps):\n",
    "            # Compute the stumps weigthed errors.\n",
    "            EP = np.matmul(w, E) # dimensions: 1 x [amount of stumps]\n",
    "\n",
    "            # Select the best stump and store it on H\n",
    "            s = np.argmin(EP)\n",
    "            H.append(S[s])\n",
    "            if s == 2 * c * n:\n",
    "                # Indicate if the stump always return false...\n",
    "                Hf.append(-1)\n",
    "            elif s == 2 * c * n + 1:\n",
    "                # ..., always return true...\n",
    "                Hf.append(1)\n",
    "            else:\n",
    "                # ...or if it is a regular stump.\n",
    "                Hf.append(0)\n",
    "            \n",
    "            # Compute the selected stump importance.\n",
    "            et = np.amin(EP)\n",
    "            if et == 0:\n",
    "                # If the stump error was 0, it means it is strong for this dataset.\n",
    "                # If so, we set its importance to one, and stop selecting stumps here.\n",
    "                # This case is always going to happen on the first iteration, as we\n",
    "                # always show the full dataset to each stump.\n",
    "                A.append(1)\n",
    "                break\n",
    "            # If the stump made mistakes, then we calculate the importance in the\n",
    "            # regular way and add it to A.\n",
    "            alpha = (1 / 2) * math.log((1 - et) / et)\n",
    "            A.append(alpha)\n",
    "\n",
    "            # Knowing the mistakes that the selected stump made, we need to update the\n",
    "            # weights so we select a stump that is good on what this one is bad at.\n",
    "            w *= np.vectorize(math.exp)(-1 * alpha * ((((E[:, s] == y) + 0) * 2) - 1))\n",
    "            w /= np.sum(w)\n",
    "\n",
    "        # Now that we selected the stumps, we store them for when we need to predict.\n",
    "        self.A_ = A\n",
    "        self.H_ = H\n",
    "        self.Hf_ = Hf\n",
    "            \n",
    "        # Return the classifier.\n",
    "        return self\n",
    "\n",
    "    # The same assumption is made in here. X will be a matrix of positive integers.\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called.\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        # Input validation.\n",
    "        X = check_array(X, ensure_2d=False, dtype='numeric')\n",
    "\n",
    "        # We follow the same process to get P with 0s where the stumps returned false,\n",
    "        # and 1s where the stumps returned true.\n",
    "        P = self.compute_stump_predictions_(X, np.array(self.H_))\n",
    "\n",
    "        # But now we want it slightly different to make it easier to compute the final\n",
    "        # prediction. We keep 1s as they are, but we transform 0s to -1s.\n",
    "        P = ((P * 2) - 1)\n",
    "\n",
    "        # We also need to set fixed responses for stumps that always return true or\n",
    "        # always return false. We do so with the Hf array we built during training.\n",
    "        Hf_np = np.array(self.Hf_)\n",
    "        m, _ = X.shape\n",
    "        P[:, Hf_np != 0] = np.tile(Hf_np[Hf_np != 0], (m, 1))\n",
    "\n",
    "        # To compute the predictions, we sum each stump prediction considering the\n",
    "        # importances we calculated during training. If the result is negative, then\n",
    "        # we predict 0. If the result is positive, then we predict 1.\n",
    "        y = np.matmul(P, np.array(self.A_))\n",
    "        y[y >= 0] = 1\n",
    "        y[y < 0] = 0\n",
    "\n",
    "        # Finally return the prediction.\n",
    "        return y\n",
    "\n",
    "# Create some simple data for testing the implementation\n",
    "dummy_X = np.array([\n",
    "    [1, 2],\n",
    "    [2, 2],\n",
    "    [2, 1]\n",
    "])\n",
    "dummy_y = np.array([\n",
    "    1,\n",
    "    0,\n",
    "    1\n",
    "])\n",
    "dummy_predict_X = np.array([\n",
    "    [1, 1],\n",
    "    [2, 2]\n",
    "])\n",
    "\n",
    "# Test the implementation.\n",
    "Tp2ClassifierTake2(n_stumps=1).fit(dummy_X, dummy_y).predict(dummy_predict_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação e Performance\n",
    "\n",
    "Na segunda tentativa, podemos observar tempos extremamente menores graças ao uso de operações matriciais. Como mencionado, agora é possível avaliar o algoritmo variando o parâmetro de quantidade de stumps selecionados. Os valores utilizados foram de 1, 5, 10, 50, 100, 500, e 1000 stumps.\n",
    "\n",
    "Os dados resultantes de tal experimento infelizmente são inconclusivos. A quantidade de stumps selecionados não teve efeito na acurácia do modelo gerado. Claramente há um erro de implementação, e a hipótese é de que ele está no cálculo das importâncias dos stumps e da atualização dos pesos para cada exemplo. Tal erro não foi identificado a tempo da entrega do trabalho."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tic tac toe dataset\n",
    "tic_tac_toe_data = pd.read_csv('./tic-tac-toe/tic-tac-toe.data', header=None)\n",
    "\n",
    "# Transform the features categories from strings to positive integers\n",
    "X = tic_tac_toe_data.drop([9], axis=1)\n",
    "X = X.apply(LabelEncoder().fit_transform) + 1\n",
    "\n",
    "# Transform the classes from strings to 0s and 1s\n",
    "y = tic_tac_toe_data[9]\n",
    "y = label_binarize(y, classes=['negative', 'positive'], pos_label = 1, neg_label = 0)\n",
    "y = y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70 (+/- 0.10)\n",
      "CPU times: user 142 ms, sys: 5.84 ms, total: 148 ms\n",
      "Wall time: 45.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake2(n_stumps=1), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70 (+/- 0.10)\n",
      "CPU times: user 157 ms, sys: 5.23 ms, total: 162 ms\n",
      "Wall time: 45.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake2(n_stumps=5), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70 (+/- 0.10)\n",
      "CPU times: user 174 ms, sys: 5 ms, total: 179 ms\n",
      "Wall time: 49.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake2(n_stumps=10), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.70 (+/- 0.10)\n",
      "CPU times: user 365 ms, sys: 9.22 ms, total: 374 ms\n",
      "Wall time: 104 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake2(n_stumps=50), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.00)\n",
      "CPU times: user 525 ms, sys: 8.99 ms, total: 534 ms\n",
      "Wall time: 140 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake2(n_stumps=100), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.00)\n",
      "CPU times: user 1.97 s, sys: 14.6 ms, total: 1.99 s\n",
      "Wall time: 505 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake2(n_stumps=500), X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.65 (+/- 0.00)\n",
      "CPU times: user 3.68 s, sys: 23.4 ms, total: 3.71 s\n",
      "Wall time: 933 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "eval_classifier_performance(Tp2ClassifierTake2(n_stumps=1000), X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "A implementação de algoritmos de aprendizagem de máquina deve considerar a grande quantidade de operações realizadas durante o treinamento. Uma implementação que envolve o uso excessivo de estruturas de repetição e controle se provou ineficiente e inutilizável. A adoção de operações matriciais fez grande diferença, uma vez que a execução do treinamento se tornou rápida e viabilizou a realização de experimentos para avaliar a eficácia dos modelos gerados.\n",
    "\n",
    "Infelizmente não foi possível corrigir a segunda implementação, e por isso não foi possível fazer avaliações conclusivas sobre a eficácia dos modelos criados. Porém, pode-se concluir que a estragégia de Boosting para aprendizado é poderosa, uma vez que pode-se ver acurácias de até 70% conseguidas através de classificadores tão fracos quanto stumps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
